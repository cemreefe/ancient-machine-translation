{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w, akk=False):\n",
    "    w = unicode_to_ascii(str(w).lower().strip())\n",
    "\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, -), and \"-\" with \" -\".\n",
    "    w = re.sub(r\"[^a-zA-Z0-9-\\^…]+\", \" \", w)\n",
    "    \n",
    "    # DON'T TOKENIZE BY AKKADIAN TIRETS\n",
    "    #w = re.sub(r\"-\", \" -\", w)\n",
    "        \n",
    "    w = re.sub(r\"[0-9] lines missing|[0-9] lines fragmentary|unknown no of lines missing\", \"…\", w)\n",
    "    \n",
    "    if akk:\n",
    "        w = re.sub(r\"x\", \"…\", w)\n",
    "    else:\n",
    "        w = re.sub(r\"[- ]+\", \" \", w)\n",
    "        \n",
    "    w = re.sub(r\"…+\", \"…\", w)\n",
    "    w = re.sub(r\"(… )+\", \"… \", w)\n",
    "    w = re.sub(r\"( …)+\", \" …\", w)\n",
    "\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pairs = pd.read_csv(\"data/6882pairs.csv\", \";\")\n",
    "pairs = pairs.reindex(pairs.akk.str.len().sort_values().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "en_sentence = pairs[\"en\"][t] #u\"May I borrow this book?\"\n",
    "akk_sentence = pairs[\"akk\"][t] #u\"¿Puedo tomar prestado este libro?\"\n",
    "print(en_sentence +\"\\n\")\n",
    "print(akk_sentence+\"\\n\")\n",
    "print(preprocess_sentence(en_sentence)+\"\\n\")\n",
    "print(preprocess_sentence(akk_sentence, akk=True)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_en  = lambda t: preprocess_sentence(t)\n",
    "prep_akk = lambda t: preprocess_sentence(t, akk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit =  -1166\n",
    "en    =  np.array([prep_en(xi) for xi in np.array(pairs[\"en\"])])[:limit]\n",
    "akk   =  np.array([prep_akk(xi) for xi in np.array(pairs[\"akk\"])])[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists\n",
    "en_l = []\n",
    "akk_l = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in en:\n",
    "      en_l.append(len(i.split()))\n",
    "\n",
    "for i in akk:\n",
    "      akk_l.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'en':en_l, 'akk':akk_l})\n",
    "\n",
    "length_df.hist(bins = 10, range=[0,500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build a tokenizer\n",
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare english tokenizer\n",
    "en_tokenizer = tokenization(en)\n",
    "en_vocab_size = len(en_tokenizer.word_index) + 1\n",
    "\n",
    "en_length = 200\n",
    "print('English Vocabulary Size: %d' % en_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Deutch tokenizer\n",
    "akk_tokenizer = tokenization(akk)\n",
    "akk_vocab_size = len(akk_tokenizer.word_index) + 1\n",
    "\n",
    "akk_length = 200\n",
    "print('Akkadian Vocabulary Size: %d' % akk_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    return seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train and test set\n",
    "xTrain_, xTest_, yTrain_, yTest_  = train_test_split(en, akk, test_size=0.2, random_state = 12)\n",
    "xTrain = encode_sequences(en_tokenizer, en_length, xTrain_)\n",
    "yTrain = encode_sequences(akk_tokenizer, akk_length, yTrain_)\n",
    "xTest = encode_sequences(en_tokenizer, en_length, xTest_)\n",
    "yTest = encode_sequences(akk_tokenizer, akk_length, yTest_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build NMT model\n",
    "def define_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    model.add(Dense(out_vocab, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compilation\n",
    "model = define_model(en_vocab_size, akk_vocab_size, en_length, akk_length, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = optimizers.RMSprop(lr=0.001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'chkpts/model.h1'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# train model\n",
    "history = model.fit(xTrain, yTrain.reshape(yTrain.shape[0], yTrain.shape[1], 1), \n",
    "                    validation_data=[xTest, yTest.reshape(yTest.shape[0], yTest.shape[1], 1)],\n",
    "                    epochs=10, batch_size=512, validation_split = 0.2,callbacks=[checkpoint], \n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_classes(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], akk_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], akk_tokenizer)) or (t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t) \n",
    "\n",
    "    preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTest_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
